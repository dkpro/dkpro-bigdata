// Copyright 2015
// Ubiquitous Knowledge Processing (UKP) Lab and FG Language Technology
// Technische Universit√§t Darmstadt
// 
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
// 
// http://www.apache.org/licenses/LICENSE-2.0
// 
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

## Running Pipelines

### Instructions

* Write a class that extends DKProHadoopDriver
* Implement abstract methods:
** buildCollectionReader
*** Executed within the driver. 
*** Can access files on the computer where the job is started.
*** Copies data to the cluster and converts it to !SequenceFile
** buildMapperEngine
*** Executed on the compute node to construct your Analysis Engine
** configure

Moreover, you will need a main method that will use Hadoop to submit your job to the cluster. In 
general you can use the main method in below example as a template.

### Example 

[source,java]
----
public class UimaPipelineOnHadoop
    extends DkproHadoopDriver
{
  public CollectionReader buildCollectionReader()
      throws ResourceInitializationException
  {
    return createReader(TextReader.class, 
        TextReader.PARAM_SOURCE_LOCATION, "src/test/resources/text",
        TextReader.PARAM_PATTERNS, "*.txt",
        TextReader.PARAM_LANGUAGE, "en");
  }
     
  public AnalysisEngineDescription buildMapperEngine(Configuration job)
      throws ResourceInitializationException
  {
    AnalysisEngineDescription tokenizer = createEngineDescription(BreakIteratorSegmenter.class);
    AnalysisEngineDescription stemmer = createEngineDescription(SnowballStemmer.class,
        SnowballStemmer.PARAM_LANGUAGE, "en"); 
    return createEngineDescription(tokenizer, stemmer);
  }  
  
  @Override
  public void configure(JobConf job) {
    job.set("mapreduce.job.queuename", "smalljob");	
  }

  public static void main(String[] args)
      throws Exception
  {
    UimaPipelineOnHadoop pipeline = new UimaPipelineOnHadoop();
    pipeline.setMapperClass(DkproMapper.class);
    pipeline.setReducerClass(DkproReducer.class);
    ToolRunner.run(new Configuration(), pipeline, args);
  } 
}
----
